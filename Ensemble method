{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/silarin/silarin/blob/main/Ensemble%20method\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Here's a combined Python script that uses an ensemble of machine learning models, including a Recurrent Neural Network (RNN), a Convolutional Neural Network (CNN), and a Generative Adversarial Network (GAN), to analyze winning lotto numbers and predict the next possible set of numbers:\n",
        "\n",
        "#```python\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Conv1D, LSTM, GRU, Dropout, LeakyReLU\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define the valid range of lotto numbers\n",
        "MIN_NUMBER = 1\n",
        "MAX_NUMBER = 49\n",
        "NUM_NUMBERS = 7\n",
        "\n",
        "# Load and preprocess the historical lotto data\n",
        "# (Replace this with your data loading and preprocessing code)\n",
        "historical_data = np.random.randint(MIN_NUMBER, MAX_NUMBER + 1, size=(1000, NUM_NUMBERS))\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(historical_data[:-1], historical_data[1:], test_size=0.2, shuffle=False)\n",
        "\n",
        "# Normalize the data\n",
        "X_train = X_train / (MAX_NUMBER - MIN_NUMBER + 1)\n",
        "X_test = X_test / (MAX_NUMBER - MIN_NUMBER + 1)\n",
        "y_train = y_train / (MAX_NUMBER - MIN_NUMBER + 1)\n",
        "y_test = y_test / (MAX_NUMBER - MIN_NUMBER + 1)\n",
        "\n",
        "# Define the RNN model\n",
        "rnn_model = Sequential([\n",
        "    LSTM(64, input_shape=(NUM_NUMBERS, 1), return_sequences=True),\n",
        "    Dropout(0.2),\n",
        "    LSTM(64, return_sequences=True),\n",
        "    Dropout(0.2),\n",
        "    LSTM(64),\n",
        "    Dropout(0.2),\n",
        "    Dense(NUM_NUMBERS * (MAX_NUMBER - MIN_NUMBER + 1), activation='softmax')\n",
        "])\n",
        "\n",
        "# Define the CNN model\n",
        "cnn_model = Sequential([\n",
        "    Conv1D(64, 3, padding='same', activation='relu', input_shape=(NUM_NUMBERS, 1)),\n",
        "    Dropout(0.2),\n",
        "    Conv1D(64, 3, padding='same', activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Conv1D(64, 3, padding='same', activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(NUM_NUMBERS * (MAX_NUMBER - MIN_NUMBER + 1), activation='softmax')\n",
        "])\n",
        "\n",
        "# Define the GAN architecture\n",
        "def build_generator(latent_dim):\n",
        "    inputs = Input(shape=(latent_dim,))\n",
        "    x = Dense(128 * NUM_NUMBERS)(inputs)\n",
        "    x = LeakyReLU()(x)\n",
        "    x = Reshape((NUM_NUMBERS, 128))(x)\n",
        "    x = Conv1D(64, 3, padding='same')(x)\n",
        "    x = LeakyReLU()(x)\n",
        "    x = Conv1D(32, 3, padding='same')(x)\n",
        "    x = LeakyReLU()(x)\n",
        "    x = Conv1D(1, 3, padding='same', activation='tanh')(x)\n",
        "    x = Reshape((NUM_NUMBERS,))(x)\n",
        "    outputs = (x + 1) * (MAX_NUMBER - MIN_NUMBER + 1) / 2 + MIN_NUMBER - 0.5\n",
        "    return Model(inputs, outputs)\n",
        "\n",
        "def build_discriminator():\n",
        "    inputs = Input(shape=(NUM_NUMBERS,))\n",
        "    x = Reshape((NUM_NUMBERS, 1))(inputs)\n",
        "    x = Conv1D(32, 3, padding='same')(x)\n",
        "    x = LeakyReLU()(x)\n",
        "    x = Conv1D(64, 3, padding='same')(x)\n",
        "    x = LeakyReLU()(x)\n",
        "    x = Flatten()(x)\n",
        "    x = Dense(128)(x)\n",
        "    x = LeakyReLU()(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    outputs = Dense(1, activation='sigmoid')(x)\n",
        "    return Model(inputs, outputs)\n",
        "\n",
        "# Build and compile the GAN\n",
        "latent_dim = 100\n",
        "generator = build_generator(latent_dim)\n",
        "discriminator = build_discriminator()\n",
        "\n",
        "discriminator.compile(optimizer=Adam(), loss='binary_crossentropy')\n",
        "\n",
        "discriminator.trainable = False\n",
        "inputs = Input(shape=(latent_dim,))\n",
        "generated_numbers = generator(inputs)\n",
        "validity = discriminator(generated_numbers)\n",
        "gan = Model(inputs, validity)\n",
        "gan.compile(optimizer=Adam(), loss='binary_crossentropy')\n",
        "\n",
        "# Compile and train the models\n",
        "rnn_model.compile(optimizer=Adam(), loss=BinaryCrossentropy())\n",
        "cnn_model.compile(optimizer=Adam(), loss=BinaryCrossentropy())\n",
        "\n",
        "rnn_model.fit(X_train.reshape(-1, NUM_NUMBERS, 1), y_train.reshape(-1, NUM_NUMBERS * (MAX_NUMBER - MIN_NUMBER + 1)), epochs=100, batch_size=32, validation_data=(X_test.reshape(-1, NUM_NUMBERS, 1), y_test.reshape(-1, NUM_NUMBERS * (MAX_NUMBER - MIN_NUMBER + 1))))\n",
        "cnn_model.fit(X_train.reshape(-1, NUM_NUMBERS, 1), y_train.reshape(-1, NUM_NUMBERS * (MAX_NUMBER - MIN_NUMBER + 1)), epochs=100, batch_size=32, validation_data=(X_test.reshape(-1, NUM_NUMBERS, 1), y_test.reshape(-1, NUM_NUMBERS * (MAX_NUMBER - MIN_NUMBER + 1))))\n",
        "\n",
        "epochs = 100\n",
        "batch_size = 32\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for batch in range(historical_data.shape[0] // batch_size):\n",
        "        noise = np.random.normal(0, 1, size=(batch_size, latent_dim))\n",
        "        generated_numbers = generator.predict(noise)\n",
        "\n",
        "        real_numbers = historical_data[np.random.randint(0, historical_data.shape[0], size=batch_size)]\n",
        "        combined_numbers = np.concatenate([real_numbers, generated_numbers])\n",
        "        labels = np.concatenate([np.ones((batch_size, 1)), np.zeros((batch_size, 1))])\n",
        "        labels += 0.05 * np.random.random(labels.shape)\n",
        "\n",
        "        d_loss = discriminator.train_on_batch(combined_numbers, labels)\n",
        "\n",
        "        noise = np.random.normal(0, 1, size=(batch_size, latent_dim))\n",
        "        misleading_labels = np.ones((batch_size, 1))\n",
        "\n",
        "        g_loss = gan.train_on_batch(noise, misleading_labels)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Discriminator Loss: {d_loss}, Generator Loss: {g_loss}\")\n",
        "\n",
        "# Ensemble the models for prediction\n",
        "def predict_next_numbers(models):\n",
        "    predictions = []\n",
        "    for model in models:\n",
        "        prediction = model.predict(np.expand_dims(historical_data[-1] / (MAX_NUMBER - MIN_NUMBER + 1), axis=0).reshape(1, NUM_NUMBERS, 1))\n",
        "        prediction = prediction.reshape(NUM_NUMBERS, MAX_NUMBER - MIN_NUMBER + 1)\n",
        "        prediction = np.argmax(prediction, axis=1) + MIN_NUMBER\n",
        "        predictions.append(prediction)\n",
        "\n",
        "    ensemble_prediction = np.mean(predictions, axis=0)\n",
        "    ensemble_prediction = ensemble_prediction.astype(int)\n",
        "    return ensemble_prediction\n",
        "\n",
        "# Make predictions\n",
        "models = [rnn_model, cnn_model, generator]\n",
        "predicted_numbers = predict_next_numbers(models)\n",
        "print(\"Predicted next set of numbers:\")\n",
        "print(\" \".join(str(num) for num in predicted_numbers))\n",
        "#```\n",
        "\n",
        "#This combined script includes the following components:\n",
        "\n",
        "#1. Data loading and preprocessing (replace this with your own code).\n",
        "#2. Definition and training of the RNN model.\n",
        "#3. Definition and training of the CNN model.\n",
        "#4. Definition and training of the GAN model, including the generator and discriminator architectures.\n",
        "#5. An ensemble​​​​​​​​​​​​​​​​"
      ],
      "metadata": {
        "id": "-hq5AChpF6KQ",
        "outputId": "6a4f9dd2-ec6e-458b-c283-a1031ff3d6ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 631
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "cannot reshape array of size 5593 into shape (343)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a88f5b62ae95>\u001b[0m in \u001b[0;36m<cell line: 100>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0mcnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBinaryCrossentropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m \u001b[0mrnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_NUMBERS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_NUMBERS\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mMAX_NUMBER\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mMIN_NUMBER\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_NUMBERS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_NUMBERS\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mMAX_NUMBER\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mMIN_NUMBER\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0mcnn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_NUMBERS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_NUMBERS\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mMAX_NUMBER\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mMIN_NUMBER\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_NUMBERS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_NUMBERS\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mMAX_NUMBER\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mMIN_NUMBER\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 5593 into shape (343)"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}